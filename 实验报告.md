


github地址：https://github.com/bouncekk/project5-multimodal-sentiment

git clone https://github.com/bouncekk/project5-multimodal-sentiment
touch datasets/__init__.py
python -m train --data_dir . --save_dir checkpoints --val_ratio 0.1 --batch_size 16 --epochs 10 --lr 1e-3 --modality both



本文在模型设计中分别选用预训练的 BERT 作为文本编码器、Vision Transformer（ViT）作为图像编码器。BERT 通过大规模语料预训练，能够有效建模文本中的上下文语义与情感倾向；而 ViT 通过自注意力机制对图像进行全局建模，相较于传统卷积网络更适合与基于 Transformer 的文本模型进行特征对齐。这种在表示层面保持结构一致性的设计，为后续跨模态注意力建模提供了更加自然和稳定的输入基础。

同时已有研究表明，简单的特征拼接难以有效建模文本与图像之间的细粒度语义关联，本文引入跨模态注意力（Cross-Modal Attention）对文本与图像特征进行显式建模。论文指出，通过显式建模不同模态之间的对齐关系，可以使模型在融合阶段动态关注与情感表达最相关的跨模态特征。

亮点？



默认使用 google/vit-base-patch16-224-in21k 的配置。
如果 pretrained=True（现在是），会从网络下载预训练权重（第一次需要联网）


References:  
1. 。。。


Chang, Y.; Li, Z.; Ruan, Y.; Yin, G. Image–Text Multimodal Sentiment Analysis Algorithm Based on Curriculum Learning and Attention Mechanisms. Big Data Cogn. Comput. 2026, 10, 23. https://doi.org/10.3390/bdcc10010023     

https://github.com/declare-lab/multimodal-deep-learning

