


github地址：https://github.com/bouncekk/project5-multimodal-sentiment

git clone https://github.com/bouncekk/project5-multimodal-sentiment.git
wget https://github.com/bouncekk/project5-multimodal-sentiment/archive/refs/heads/main.zip
unzip main.zip
unzip models--google--vit-base-patch16-224-in21k.zip
touch datasets/__init__.py
ImageEncoder
self.vit = ViTModel.from_pretrained(
    "/mnt/workspace/project5-multimodal-sentiment/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0"
)



python train.py --data_dir . --save_dir checkpoints_bert_vit --val_ratio 0.1 --batch_size 8 --epochs 10 --lr 1e-4 --modality both

      解决git问题。试试纯图像，问both是为什么。可问gpt。调整超参数看看。提高参数量。然后创新了。也能直接创新。

本文在模型设计中分别选用预训练的 BERT 作为文本编码器、Vision Transformer（ViT）作为图像编码器。BERT 通过大规模语料预训练，能够有效建模文本中的上下文语义与情感倾向；而 ViT 通过自注意力机制对图像进行全局建模，相较于传统卷积网络更适合与基于 Transformer 的文本模型进行特征对齐。这种在表示层面保持结构一致性的设计，为后续跨模态注意力建模提供了更加自然和稳定的输入基础。

同时已有研究表明，简单的特征拼接难以有效建模文本与图像之间的细粒度语义关联，本文引入跨模态注意力（Cross-Modal Attention）对文本与图像特征进行显式建模。论文指出，通过显式建模不同模态之间的对齐关系，可以使模型在融合阶段动态关注与情感表达最相关的跨模态特征。


改：ViLBERT 作为一种典型的双流视觉-语言预训练模型，已在多个多模态理解任务中验证了其有效性。然而，其跨模态融合机制已深度内嵌于模型结构之中，融合过程主要依赖于预训练阶段学习到的跨模态注意力权重。对于本实验而言，研究重点在于分析不同融合策略对多模态情感分类性能的影响，而非直接复现或微调现有的大规模视觉-语言预训练模型。因此，若直接采用 ViLBERT，将难以对融合过程进行显式控制和模块化分析，不利于对融合方法本身的实验设计与对比。

亮点？



默认使用 google/vit-base-patch16-224-in21k 的配置。
如果 pretrained=True（现在是），会从网络下载预训练权重（第一次需要联网）


References:  
[1] Hamidi M A M, Taqa A Y, Ibrahim Y I.
A Systematic Review of Multimodal Sentiment Analysis Based on Text-Image Fusion: Trends, Models, and Research Gaps[J].
Sinkron: Jurnal dan Penelitian Teknik Informatika, 2025, 9(2).
DOI: 10.33395/sinkron.v9i2.14840.


[2]
Chang, Y.; Li, Z.; Ruan, Y.; Yin, G. Image–Text Multimodal Sentiment Analysis Algorithm Based on Curriculum Learning and Attention Mechanisms. Big Data Cogn. Comput. 2026, 10, 23. https://doi.org/10.3390/bdcc10010023     

[3]
https://github.com/declare-lab/multimodal-deep-learning



用的非预训练vit

基础版：
cross_modal_attention:62     to 64 io 62













除了基础的文本清洗（编码错误忽略、空白去除、索引化）与图像归一化外，后续工作中可以进一步引入更丰富的数据增强策略。例如在图像模态中加入随机翻转、轻微随机裁剪或颜色抖动等操作，以提升模型对视觉噪声和视角变化的鲁棒性；在文本模态中可以考虑更加精细的分词与停用词处理，从而进一步降低无关噪声对情感判别的干扰。受限于时间与算力，本实验未对上述策略逐一展开系统对比，仅在当前基础预处理设置下完成主要实验。


在整体架构上，本文采用“文本编码器 + 图像编码器 + 跨模态注意力融合模块 + 分类器”的分层设计。各部分在训练过程中的实际使用方式如下。

文本编码器（Text Encoder）
文本部分采用 BERT 风格的 Transformer 编码器结构。具体而言，输入的 input_ids 和 attention_mask 先通过词嵌入层（token embedding）与位置嵌入层（position embedding）相加得到序列表示，随后依次经过若干层自注意力编码器层（Multi-Head Self-Attention + 前馈网络 + 残差连接与 LayerNorm）。模型最终使用每条序列中 [CLS] 位置对应的隐藏状态向量作为整句文本的语义表示，得到固定维度的文本特征向量 text_feat。
图像编码器（Image Encoder）
图像部分采用 Vision Transformer（ViT）作为骨干网络。输入图像首先被划分为若干固定大小的图像 patch，每个 patch 通过线性投影映射到特征向量空间，并加上可学习的位置编码，形成 patch 序列。该序列依次通过多层 Transformer 编码器层（与文本侧结构类似），其中包括多头自注意力子层和前馈子层。最终使用 ViT 的 CLS token 输出或池化后的全局特征作为整张图像的表示，得到图像特征向量 img_feat。
跨模态注意力融合模块（Cross-Modal Attention Fusion）
在多模态设置下，模型首先将文本特征和图像特征分别通过线性投影映射到统一的隐空间维度，然后构造跨模态注意力层：
以文本特征作为 Query，图像特征作为 Key 与 Value，使用多头注意力（Multi-Head Attention）计算文本对图像信息的加权聚合；
得到的注意力输出再经过一层前馈网络（Feed-Forward Network）和非线性激活，实现模态间的深度交互；
从而得到融合后的多模态表示向量 fused_feat，作为后续分类的输入。
分类器（Classifier Head）
分类阶段使用一层或多层全连接网络（MLP）作为解码头：
对于 text_only 实验，以 text_feat 作为输入，送入 MLP，输出 3 维的情感 logits；
对于 image_only 实验，以 img_feat 作为输入，送入同一结构的 MLP 进行三分类；
对于 both（多模态）实验，以融合后的 fused_feat 作为输入，经过 MLP 输出最终的情感预测。
在训练过程中，分类器输出与真实标签之间通过交叉熵损失（Cross-Entropy Loss）计算目标函数，利用反向传播同时更新文本编码器、图像编码器、跨模态注意力模块与分类头的参数（在部分消融设置中也尝试冻结图像主干，仅训练分类头）。


两个clip分析：可放gpt。不过估计只用第二种了。
可画图。


完整 CLIP 方法基于对比学习在 embedding 空间中对齐图像与文本，其训练目标与情感分类任务存在显著差异，因此本文未直接采用该范式，
而是选择：




在对比 BERT+ViT 模型与 CLIP 编码器模型时，除底层编码器结构外，其余超参数（隐空间维度、融合模块与分类头结构、优化器与训练轮数等）保持一致，以保证对比结果的可解释性和公平性。


clip融合 96.306 M, 69



接口（预训练版）    