


github地址：https://github.com/bouncekk/project5-multimodal-sentiment

git clone https://github.com/bouncekk/project5-multimodal-sentiment
touch datasets/__init__.py
python -m train --data_dir . --save_dir checkpoints --val_ratio 0.1 --batch_size 16 --epochs 10 --lr 1e-3 --modality both

改成本地模型。

本文在模型设计中分别选用预训练的 BERT 作为文本编码器、Vision Transformer（ViT）作为图像编码器。BERT 通过大规模语料预训练，能够有效建模文本中的上下文语义与情感倾向；而 ViT 通过自注意力机制对图像进行全局建模，相较于传统卷积网络更适合与基于 Transformer 的文本模型进行特征对齐。这种在表示层面保持结构一致性的设计，为后续跨模态注意力建模提供了更加自然和稳定的输入基础。

同时已有研究表明，简单的特征拼接难以有效建模文本与图像之间的细粒度语义关联，本文引入跨模态注意力（Cross-Modal Attention）对文本与图像特征进行显式建模。论文指出，通过显式建模不同模态之间的对齐关系，可以使模型在融合阶段动态关注与情感表达最相关的跨模态特征。


改：ViLBERT 作为一种典型的双流视觉-语言预训练模型，已在多个多模态理解任务中验证了其有效性。然而，其跨模态融合机制已深度内嵌于模型结构之中，融合过程主要依赖于预训练阶段学习到的跨模态注意力权重。对于本实验而言，研究重点在于分析不同融合策略对多模态情感分类性能的影响，而非直接复现或微调现有的大规模视觉-语言预训练模型。因此，若直接采用 ViLBERT，将难以对融合过程进行显式控制和模块化分析，不利于对融合方法本身的实验设计与对比。

亮点？



默认使用 google/vit-base-patch16-224-in21k 的配置。
如果 pretrained=True（现在是），会从网络下载预训练权重（第一次需要联网）


References:  
1. 。。。


Chang, Y.; Li, Z.; Ruan, Y.; Yin, G. Image–Text Multimodal Sentiment Analysis Algorithm Based on Curriculum Learning and Attention Mechanisms. Big Data Cogn. Comput. 2026, 10, 23. https://doi.org/10.3390/bdcc10010023     

https://github.com/declare-lab/multimodal-deep-learning






用的非预训练vit



